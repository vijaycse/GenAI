{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:36:25.050179Z","iopub.execute_input":"2025-04-02T11:36:25.050636Z","iopub.status.idle":"2025-04-02T11:36:26.648176Z","shell.execute_reply.started":"2025-04-02T11:36:25.050592Z","shell.execute_reply":"2025-04-02T11:36:26.646546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -qqy jupyterlab  # Remove unused packages from Kaggle's base image that conflict\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:36:26.649829Z","iopub.execute_input":"2025-04-02T11:36:26.650492Z","iopub.status.idle":"2025-04-02T11:36:36.245082Z","shell.execute_reply.started":"2025-04-02T11:36:26.650443Z","shell.execute_reply":"2025-04-02T11:36:36.243638Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\nfrom IPython.display import HTML, Markdown, display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:37:53.277143Z","iopub.execute_input":"2025-04-02T11:37:53.277485Z","iopub.status.idle":"2025-04-02T11:37:53.282397Z","shell.execute_reply.started":"2025-04-02T11:37:53.277461Z","shell.execute_reply":"2025-04-02T11:37:53.281066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google.api_core import retry\n\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\ngenai.models.Models.generate_content = retry.Retry(\n    predicate=is_retriable)(genai.models.Models.generate_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:37:55.564593Z","iopub.execute_input":"2025-04-02T11:37:55.56502Z","iopub.status.idle":"2025-04-02T11:37:55.852149Z","shell.execute_reply.started":"2025-04-02T11:37:55.564986Z","shell.execute_reply":"2025-04-02T11:37:55.85107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:37:59.193299Z","iopub.execute_input":"2025-04-02T11:37:59.193929Z","iopub.status.idle":"2025-04-02T11:37:59.365437Z","shell.execute_reply.started":"2025-04-02T11:37:59.193887Z","shell.execute_reply":"2025-04-02T11:37:59.364271Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"client = genai.Client(api_key=GOOGLE_API_KEY)\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=\"Explain AI to me like I'm a kid.\")\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:38:01.545908Z","iopub.execute_input":"2025-04-02T11:38:01.546442Z","iopub.status.idle":"2025-04-02T11:38:03.865335Z","shell.execute_reply.started":"2025-04-02T11:38:01.546391Z","shell.execute_reply":"2025-04-02T11:38:03.864085Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Markdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:38:07.037017Z","iopub.execute_input":"2025-04-02T11:38:07.037397Z","iopub.status.idle":"2025-04-02T11:38:07.046202Z","shell.execute_reply.started":"2025-04-02T11:38:07.037366Z","shell.execute_reply":"2025-04-02T11:38:07.044884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"chat = client.chats.create(model='gemini-2.0-flash', history=[])\nresponse = chat.send_message('Hello! My name is Zlork.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:38:11.153408Z","iopub.execute_input":"2025-04-02T11:38:11.153764Z","iopub.status.idle":"2025-04-02T11:38:12.029055Z","shell.execute_reply.started":"2025-04-02T11:38:11.153712Z","shell.execute_reply":"2025-04-02T11:38:12.027629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"response = chat.send_message('Can you tell me something interesting about dinosaurs?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:38:46.498036Z","iopub.execute_input":"2025-04-02T11:38:46.498446Z","iopub.status.idle":"2025-04-02T11:38:47.870527Z","shell.execute_reply.started":"2025-04-02T11:38:46.498415Z","shell.execute_reply":"2025-04-02T11:38:47.869326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for model in client.models.list():\n  print(model.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:38:51.825424Z","iopub.execute_input":"2025-04-02T11:38:51.825903Z","iopub.status.idle":"2025-04-02T11:38:51.894246Z","shell.execute_reply.started":"2025-04-02T11:38:51.825867Z","shell.execute_reply":"2025-04-02T11:38:51.893058Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pprint import pprint\n\nfor model in client.models.list():\n  if model.name == 'models/gemini-2.0-flash':\n    pprint(model.to_json_dict())\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:41:08.93335Z","iopub.execute_input":"2025-04-02T11:41:08.933791Z","iopub.status.idle":"2025-04-02T11:41:09.191786Z","shell.execute_reply.started":"2025-04-02T11:41:08.933724Z","shell.execute_reply":"2025-04-02T11:41:09.190426Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google.genai import types\n\nshort_config = types.GenerateContentConfig(max_output_tokens=200)\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=short_config,\n    contents='Write a 1000 word essay on the importance of olives in modern society.')\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:41:12.120263Z","iopub.execute_input":"2025-04-02T11:41:12.120601Z","iopub.status.idle":"2025-04-02T11:41:13.524447Z","shell.execute_reply.started":"2025-04-02T11:41:12.120575Z","shell.execute_reply":"2025-04-02T11:41:13.522782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"high_temp_config = types.GenerateContentConfig(temperature=2.0)\n\n\nfor _ in range(5):\n  response = client.models.generate_content(\n      model='gemini-2.0-flash',\n      config=high_temp_config,\n      contents='Pick a random colour... (respond in a single word)')\n\n  if response.text:\n    print(response.text, '-' * 25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:42:15.796395Z","iopub.execute_input":"2025-04-02T11:42:15.796833Z","iopub.status.idle":"2025-04-02T11:42:17.902351Z","shell.execute_reply.started":"2025-04-02T11:42:15.796795Z","shell.execute_reply":"2025-04-02T11:42:17.901064Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"low_temp_config = types.GenerateContentConfig(temperature=0.0)\n\nfor _ in range(5):\n  response = client.models.generate_content(\n      model='gemini-2.0-flash',\n      config=low_temp_config,\n      contents='Pick a random colour... (respond in a single word)')\n\n  if response.text:\n    print(response.text, '-' * 25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:42:43.564681Z","iopub.execute_input":"2025-04-02T11:42:43.56513Z","iopub.status.idle":"2025-04-02T11:42:45.641611Z","shell.execute_reply.started":"2025-04-02T11:42:43.5651Z","shell.execute_reply":"2025-04-02T11:42:45.640219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_config = types.GenerateContentConfig(\n    # These are the default values for gemini-2.0-flash.\n    temperature=1.0,\n    top_p=1,\n)\n\nstory_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=model_config,\n    contents=story_prompt)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:44:57.385244Z","iopub.execute_input":"2025-04-02T11:44:57.385582Z","iopub.status.idle":"2025-04-02T11:45:02.59858Z","shell.execute_reply.started":"2025-04-02T11:44:57.385557Z","shell.execute_reply":"2025-04-02T11:45:02.597057Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**zero prompting**\n\nZero-shot prompts are prompts that describe the request for the model directly.","metadata":{}},{"cell_type":"code","source":"model_config = types.GenerateContentConfig(\n    temperature=0.1,\n    top_p=1,\n    max_output_tokens=5,\n)\n\nzero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\nReview: \"Her\" is a disturbing study revealing the direction\nhumanity is headed if AI is allowed to keep evolving,\nunchecked. I wish there were more movies like this masterpiece.\nSentiment: \"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=model_config,\n    contents=zero_shot_prompt)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:45:59.182109Z","iopub.execute_input":"2025-04-02T11:45:59.182438Z","iopub.status.idle":"2025-04-02T11:45:59.580089Z","shell.execute_reply.started":"2025-04-02T11:45:59.182414Z","shell.execute_reply":"2025-04-02T11:45:59.578971Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**ENUM** Without ENUM MODE, the prior example might lead to more output than asked\nENUM MODE restricts the output token","metadata":{}},{"cell_type":"code","source":"import enum\n\nclass Sentiment(enum.Enum):\n    POSITIVE = \"positive\"\n    NEUTRAL = \"neutral\"\n    NEGATIVE = \"negative\"\n\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        response_mime_type=\"text/x.enum\",\n        response_schema=Sentiment\n    ),\n    contents=zero_shot_prompt)\n\nprint(response.text)\n\n\nenum_response = response.parsed\nprint(enum_response)\nprint(type(enum_response))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:49:53.037646Z","iopub.execute_input":"2025-04-02T11:49:53.038179Z","iopub.status.idle":"2025-04-02T11:49:53.634404Z","shell.execute_reply.started":"2025-04-02T11:49:53.038132Z","shell.execute_reply":"2025-04-02T11:49:53.633321Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"One-shot and few-shot¶\nProviding an example of the expected response is known as a \"one-shot\" prompt. When you provide multiple examples, it is a \"few-shot\" prompt.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n\nEXAMPLE:\nI want a small pizza with cheese, tomato sauce, and pepperoni.\nJSON Response:\n```\n{\n\"size\": \"small\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n}\n```\n\nEXAMPLE:\nCan I get a large pizza with tomato sauce, basil and mozzarella\nJSON Response:\n```\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n}\n```\n\nORDER:\n\"\"\"\n\ncustomer_order = \"Give me a large with cheese & pineapple\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=0.1,\n        top_p=1,\n        max_output_tokens=250,\n    ),\n    contents=[few_shot_prompt, customer_order])\n\nprint(response.text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:52:37.689402Z","iopub.execute_input":"2025-04-02T11:52:37.689814Z","iopub.status.idle":"2025-04-02T11:52:38.316708Z","shell.execute_reply.started":"2025-04-02T11:52:37.689782Z","shell.execute_reply":"2025-04-02T11:52:38.315364Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import typing_extensions as typing\n\nclass PizzaOrder(typing.TypedDict):\n    size: str\n    ingredients: list[str]\n    type: str\n\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=0.1,\n        response_mime_type=\"application/json\",\n        response_schema=PizzaOrder,\n    ),\n    contents=\"Can I have a large dessert pizza with apple and chocolate\")\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:52:52.937629Z","iopub.execute_input":"2025-04-02T11:52:52.93808Z","iopub.status.idle":"2025-04-02T11:52:53.720375Z","shell.execute_reply.started":"2025-04-02T11:52:52.938045Z","shell.execute_reply":"2025-04-02T11:52:53.719137Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Chain of Thought (CoT)¶\nDirect prompting on LLMs can return answers quickly and (in terms of output token usage) efficiently, but they can be prone to hallucination. \nThe answer may \"look\" correct (in terms of language and syntax) but is incorrect in terms of factuality and reasoning.\n\nChain-of-Thought prompting is a technique where you instruct the model to output intermediate reasoning steps, and it typically gets better results, especially when combined with few-shot examples. It is worth noting that this technique doesn't completely eliminate hallucinations, and that it tends to cost more to run, due to the increased token count.","metadata":{}},{"cell_type":"code","source":"#NO COT\n  \nprompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\nam 20 years old. How old is my partner? Return the answer directly.\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=prompt)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:55:15.390061Z","iopub.execute_input":"2025-04-02T11:55:15.390395Z","iopub.status.idle":"2025-04-02T11:55:15.834245Z","shell.execute_reply.started":"2025-04-02T11:55:15.390369Z","shell.execute_reply":"2025-04-02T11:55:15.832989Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\nI am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=prompt)\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:55:27.252777Z","iopub.execute_input":"2025-04-02T11:55:27.253154Z","iopub.status.idle":"2025-04-02T11:55:28.370592Z","shell.execute_reply.started":"2025-04-02T11:55:27.253126Z","shell.execute_reply":"2025-04-02T11:55:28.369419Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ReAct: Reason and act¶\nIn this example you will run a ReAct prompt directly in the Gemini API and perform the searching steps yourself. As this prompt follows a well-defined structure, there are frameworks available that wrap the prompt into easier-to-use APIs that make tool calls automatically, such as the LangChain example from the \"Prompting\" whitepaper.","metadata":{}},{"cell_type":"code","source":"model_instructions = \"\"\"\nSolve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\nObservation is understanding relevant information from an Action's output and Action can be one of three types:\n (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n     will return some similar entities to search and you can try to search the information from those topics.\n (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n     so keep your searches short.\n (3) <finish>answer</finish>, which returns the answer and finishes the task.\n\"\"\"\n\nexample1 = \"\"\"Question\nMusician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n\nThought 1\nThe question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n\nAction 1\n<search>Milhouse</search>\n\nObservation 1\nMilhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n\nThought 2\nThe paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n\nAction 2\n<lookup>named after</lookup>\n\nObservation 2\nMilhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n\nThought 3\nMilhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n\nAction 3\n<finish>Richard Nixon</finish>\n\"\"\"\n\nexample2 = \"\"\"Question\nWhat is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n\nThought 1\nI need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n\nAction 1\n<search>Colorado orogeny</search>\n\nObservation 1\nThe Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n\nThought 2\nIt does not mention the eastern sector. So I need to look up eastern sector.\n\nAction 2\n<lookup>eastern sector</lookup>\n\nObservation 2\nThe eastern sector extends into the High Plains and is called the Central Plains orogeny.\n\nThought 3\nThe eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n\nAction 3\n<search>High Plains</search>\n\nObservation 3\nHigh Plains refers to one of two distinct land regions\n\nThought 4\nI need to instead search High Plains (United States).\n\nAction 4\n<search>High Plains (United States)</search>\n\nObservation 4\nThe High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n\nThought 5\nHigh Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n\nAction 5\n<finish>1,800 to 7,000 ft</finish>\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T12:00:15.572033Z","iopub.execute_input":"2025-04-02T12:00:15.572395Z","iopub.status.idle":"2025-04-02T12:00:15.577652Z","shell.execute_reply.started":"2025-04-02T12:00:15.572369Z","shell.execute_reply":"2025-04-02T12:00:15.576342Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To capture a single step at a time, while ignoring any hallucinated Observation steps, you will use stop_sequences to end the generation process. The steps are Thought, Action, Observation, in that order.","metadata":{}},{"cell_type":"code","source":"question = \"\"\"Question\nWho was the youngest author listed on the transformers NLP paper?\n\"\"\"\n\n# You will perform the Action; so generate up to, but not including, the Observation.\nreact_config = types.GenerateContentConfig(\n    stop_sequences=[\"\\nObservation\"],\n    system_instruction=model_instructions + example1 + example2,\n)\n\n# Create a chat that has the model instructions and examples pre-seeded.\nreact_chat = client.chats.create(\n    model='gemini-2.0-flash',\n    config=react_config,\n)\n\nresp = react_chat.send_message(question)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T12:00:33.501668Z","iopub.execute_input":"2025-04-02T12:00:33.50228Z","iopub.status.idle":"2025-04-02T12:00:34.315452Z","shell.execute_reply.started":"2025-04-02T12:00:33.502244Z","shell.execute_reply":"2025-04-02T12:00:34.314238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"observation = \"\"\"Observation 1\n[1706.03762] Attention Is All You Need\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\nWe propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n\"\"\"\nresp = react_chat.send_message(observation)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T12:01:03.705234Z","iopub.execute_input":"2025-04-02T12:01:03.705603Z","iopub.status.idle":"2025-04-02T12:01:04.866316Z","shell.execute_reply.started":"2025-04-02T12:01:03.705571Z","shell.execute_reply":"2025-04-02T12:01:04.86494Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Thinking mode¶\nThe experiemental Gemini Flash 2.0 \"Thinking\" model has been trained to generate the \"thinking process\" the model goes through as part of its response. As a result, the Flash Thinking model is capable of stronger reasoning capabilities in its responses.\n\nUsing a \"thinking mode\" model can provide you with high-quality responses without needing specialised prompting like the previous approaches. One reason this technique is effective is that you induce the model to generate relevant information (\"brainstorming\", or \"thoughts\") that is then used as part of the context in which the final response is generated.","metadata":{}},{"cell_type":"code","source":"import io\nfrom IPython.display import Markdown, clear_output\n\n\nresponse = client.models.generate_content_stream(\n    model='gemini-2.0-flash-thinking-exp',\n    contents='Who was the youngest author listed on the transformers NLP paper?',\n)\n\nbuf = io.StringIO()\nfor chunk in response:\n    buf.write(chunk.text)\n    # Display the response as it is streamed\n    print(chunk.text, end='')\n\n# And then render the finished response as formatted markdown.\nclear_output()\nMarkdown(buf.getvalue())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T12:09:38.401326Z","iopub.execute_input":"2025-04-02T12:09:38.401708Z","iopub.status.idle":"2025-04-02T12:09:44.998219Z","shell.execute_reply.started":"2025-04-02T12:09:38.40168Z","shell.execute_reply":"2025-04-02T12:09:44.997128Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Code prompting**","metadata":{}},{"cell_type":"code","source":"# The Gemini models love to talk, so it helps to specify they stick to the code if that\n# is all that you want.\ncode_prompt = \"\"\"\nWrite a Python function to calculate the factorial of a number. No explanation, provide only the code.\n\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=1,\n        top_p=1,\n        max_output_tokens=1024,\n    ),\n    contents=code_prompt)\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T12:11:14.093568Z","iopub.execute_input":"2025-04-02T12:11:14.09437Z","iopub.status.idle":"2025-04-02T12:11:14.758466Z","shell.execute_reply.started":"2025-04-02T12:11:14.094316Z","shell.execute_reply":"2025-04-02T12:11:14.757269Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Execution**","metadata":{}},{"cell_type":"code","source":"from pprint import pprint\n\nconfig = types.GenerateContentConfig(\n    tools=[types.Tool(code_execution=types.ToolCodeExecution())],\n)\n\ncode_exec_prompt = \"\"\"\nGenerate the first 14 odd prime numbers, then calculate their sum.\n\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=config,\n    contents=code_exec_prompt)\n\nfor part in response.candidates[0].content.parts:\n  pprint(part.to_json_dict())\n  print(\"-----\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T12:12:08.6266Z","iopub.execute_input":"2025-04-02T12:12:08.627165Z","iopub.status.idle":"2025-04-02T12:12:10.609466Z","shell.execute_reply.started":"2025-04-02T12:12:08.627123Z","shell.execute_reply":"2025-04-02T12:12:10.607936Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for part in response.candidates[0].content.parts:\n    if part.text:\n        display(Markdown(part.text))\n    elif part.executable_code:\n        display(Markdown(f'```python\\n{part.executable_code.code}\\n```'))\n    elif part.code_execution_result:\n        if part.code_execution_result.outcome != 'OUTCOME_OK':\n            display(Markdown(f'## Status {part.code_execution_result.outcome}'))\n\n        display(Markdown(f'```\\n{part.code_execution_result.output}\\n```'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T12:12:16.328701Z","iopub.execute_input":"2025-04-02T12:12:16.329158Z","iopub.status.idle":"2025-04-02T12:12:16.343272Z","shell.execute_reply.started":"2025-04-02T12:12:16.329126Z","shell.execute_reply":"2025-04-02T12:12:16.341878Z"}},"outputs":[],"execution_count":null}]}